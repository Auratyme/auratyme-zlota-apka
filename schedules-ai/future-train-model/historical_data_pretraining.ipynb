{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    \"\"\"\n",
    "    Returns the device to be used for tensor computations (GPU if available, else CPU).\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'Using device: {device}')\n",
    "    return device\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m---> 14\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD:/Projects/EffectiveDayAI/schedules-ai-db/Daily_Records_Dataset.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_dataset\u001b[39m(file_path):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Loads the dataset from the specified CSV file.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m        pd.DataFrame: Loaded dataset as a pandas DataFrame.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def load_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Loads the dataset from the specified CSV file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "df = load_dataset('D:/Projects/EffectiveDayAI/schedules-ai-db/Daily_Records_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_minutes(time_str):\n",
    "    \"\"\"\n",
    "    Converts a time string in 'HH:MM' format to minutes from midnight.\n",
    "\n",
    "    Args:\n",
    "        time_str (str): Time string in 'HH:MM' format.\n",
    "\n",
    "    Returns:\n",
    "        int: Minutes from midnight.\n",
    "    \"\"\"\n",
    "    h, m = map(int, time_str.split(':'))\n",
    "    return h * 60 + m\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocesses the dataset by converting time columns, encoding categorical variables,\n",
    "    and normalizing numerical features.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The raw dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The preprocessed dataset.\n",
    "    \"\"\"\n",
    "    df['Start_Minutes'] = df['Start_Time'].apply(time_to_minutes)\n",
    "    df['End_Minutes'] = df['End_Time'].apply(time_to_minutes)\n",
    "\n",
    "    df['Task_Duration'] = df['End_Minutes'] - df['Start_Minutes']\n",
    "\n",
    "    le_task_type = LabelEncoder()\n",
    "    df['Task_Type_Encoded'] = le_task_type.fit_transform(df['Task_Type'])\n",
    "\n",
    "    le_preferred_time = LabelEncoder()\n",
    "    df['Preferred_Time_Encoded'] = le_preferred_time.fit_transform(df['Preferred_Time'])\n",
    "\n",
    "    df['Fixed_Task'] = df['Fixed_Task'].astype(int)\n",
    "\n",
    "    numeric_features = ['Task_Priority', 'Task_Duration',\n",
    "                        'Breaks_Count', 'Breaks_Duration', 'Sleep_Duration', 'Sleep_Quality',\n",
    "                        'Energy_Level', 'Heart_Rate', 'Start_Minutes', 'End_Minutes']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df[numeric_features] = scaler.fit_transform(df[numeric_features])\n",
    "\n",
    "    return df\n",
    "\n",
    "df = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_columns():\n",
    "    \"\"\"\n",
    "    Defines the list of feature columns to be used in the model.\n",
    "\n",
    "    Returns:\n",
    "        list: List of feature column names.\n",
    "    \"\"\"\n",
    "    feature_columns = ['Task_Type_Encoded', 'Task_Priority', 'Task_Duration', 'Fixed_Task',\n",
    "                       'Preferred_Time_Encoded', 'Sleep_Duration', 'Sleep_Quality',\n",
    "                       'Energy_Level', 'Heart_Rate', 'Start_Minutes', 'End_Minutes']\n",
    "    return feature_columns\n",
    "\n",
    "feature_columns = get_feature_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(df, feature_columns):\n",
    "    \"\"\"\n",
    "    Groups tasks by date to create sequences and prepares per-task and overall targets.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The preprocessed dataset.\n",
    "        feature_columns (list): List of feature column names.\n",
    "\n",
    "    Returns:\n",
    "        sequences (list): List of task sequences.\n",
    "        per_task_targets (list): List of per-task target arrays.\n",
    "        overall_targets (list): List of overall target arrays.\n",
    "    \"\"\"\n",
    "    grouped = df.groupby('Date')\n",
    "\n",
    "    sequences = []\n",
    "    per_task_targets = []\n",
    "    overall_targets = []\n",
    "\n",
    "    for date, group in grouped:\n",
    "        group = group.sort_values('Start_Minutes')\n",
    "        overall_efficiency = group['Overall_Efficiency_Score'].dropna().unique()\n",
    "        overall_satisfaction = group['Overall_Satisfaction_Score'].dropna().unique()\n",
    "\n",
    "        if len(overall_efficiency) > 0 and len(overall_satisfaction) > 0:\n",
    "            overall_target = [overall_efficiency[0], overall_satisfaction[0]]\n",
    "            overall_targets.append(overall_target)\n",
    "\n",
    "            seq = group[feature_columns].values\n",
    "            sequences.append(seq)\n",
    "\n",
    "            efficiency_scores = group['Efficiency_Score'].values\n",
    "            satisfaction_scores = group['Satisfaction_Score'].values\n",
    "            per_task_target = np.column_stack((efficiency_scores, satisfaction_scores))\n",
    "            per_task_targets.append(per_task_target)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return sequences, per_task_targets, overall_targets\n",
    "\n",
    "sequences, per_task_targets, overall_targets = prepare_sequences(df, feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for task sequences and targets.\n",
    "\n",
    "    Args:\n",
    "        sequences (list): List of task sequences.\n",
    "        per_task_targets (list): List of per-task target arrays.\n",
    "        overall_targets (list): List of overall target arrays.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences, per_task_targets, overall_targets):\n",
    "        self.sequences = sequences\n",
    "        self.per_task_targets = per_task_targets\n",
    "        self.overall_targets = overall_targets\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of sequences in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the sequence and targets at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the data point.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing the sequence and targets.\n",
    "        \"\"\"\n",
    "        seq = torch.tensor(self.sequences[idx], dtype=torch.float32)\n",
    "        per_task_target = torch.tensor(self.per_task_targets[idx], dtype=torch.float32)\n",
    "        overall_target = torch.tensor(self.overall_targets[idx], dtype=torch.float32)\n",
    "        return {'sequence': seq, 'per_task_target': per_task_target, 'overall_target': overall_target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function to handle variable-length sequences and pad them.\n",
    "\n",
    "    Args:\n",
    "        batch (list): List of data points.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing padded sequences and targets.\n",
    "    \"\"\"\n",
    "    sequences = [item['sequence'] for item in batch]\n",
    "    per_task_targets = [item['per_task_target'] for item in batch]\n",
    "    overall_targets = torch.stack([item['overall_target'] for item in batch])\n",
    "\n",
    "    sequences_padded = nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n",
    "    per_task_targets_padded = nn.utils.rnn.pad_sequence(per_task_targets, batch_first=True)\n",
    "\n",
    "    return {\n",
    "        'sequence': sequences_padded,\n",
    "        'per_task_target': per_task_targets_padded,\n",
    "        'overall_target': overall_targets\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridModelWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid LSTM model with attention mechanism for predicting per-task and overall scores.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Number of input features.\n",
    "        embedding_sizes (dict): Dictionary with sizes of categorical embeddings.\n",
    "        hidden_size (int): Number of hidden units in LSTM.\n",
    "        num_layers (int): Number of LSTM layers.\n",
    "        per_task_output_size (int): Output size for per-task predictions.\n",
    "        overall_output_size (int): Output size for overall predictions.\n",
    "        dropout_prob (float): Dropout probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, embedding_sizes, hidden_size, num_layers, per_task_output_size, overall_output_size, dropout_prob):\n",
    "        super(HybridModelWithAttention, self).__init__()\n",
    "\n",
    "        self.task_type_embedding = nn.Embedding(embedding_sizes['Task_Type'], embedding_dim=16)\n",
    "        self.preferred_time_embedding = nn.Embedding(embedding_sizes['Preferred_Time'], embedding_dim=4)\n",
    "\n",
    "        lstm_input_size = 16 + 4 + (input_size - 2)\n",
    "\n",
    "        self.lstm = nn.LSTM(lstm_input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob, bidirectional=True)\n",
    "\n",
    "        self.attention_layer = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "        self.per_task_fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(64, per_task_output_size)\n",
    "        )\n",
    "\n",
    "        self.overall_fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(64, overall_output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_len, input_size].\n",
    "\n",
    "        Returns:\n",
    "            per_task_outputs (torch.Tensor): Per-task predictions of shape [batch_size, seq_len, per_task_output_size].\n",
    "            overall_output (torch.Tensor): Overall predictions of shape [batch_size, overall_output_size].\n",
    "        \"\"\"\n",
    "        task_type = x[:, :, 0].long()\n",
    "        preferred_time = x[:, :, 4].long()\n",
    "        other_features = torch.cat([x[:, :, 1:4], x[:, :, 5:]], dim=2)\n",
    "\n",
    "        task_type_embedded = self.task_type_embedding(task_type)\n",
    "        preferred_time_embedded = self.preferred_time_embedding(preferred_time)\n",
    "\n",
    "        lstm_input = torch.cat([task_type_embedded, preferred_time_embedded, other_features], dim=2)\n",
    "\n",
    "        lstm_out, _ = self.lstm(lstm_input)\n",
    "\n",
    "        per_task_outputs = self.per_task_fc(lstm_out)\n",
    "\n",
    "        attn_weights = torch.softmax(self.attention_layer(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "\n",
    "        overall_output = self.overall_fc(context_vector)\n",
    "\n",
    "        return per_task_outputs, overall_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(feature_columns, df):\n",
    "    \"\"\"\n",
    "    Initializes the model with the specified hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        feature_columns (list): List of feature column names.\n",
    "        df (pd.DataFrame): The preprocessed dataset.\n",
    "\n",
    "    Returns:\n",
    "        model (nn.Module): The initialized model.\n",
    "    \"\"\"\n",
    "    embedding_sizes = {\n",
    "        'Task_Type': df['Task_Type_Encoded'].nunique(),\n",
    "        'Preferred_Time': df['Preferred_Time_Encoded'].nunique()\n",
    "    }\n",
    "\n",
    "    input_size = len(feature_columns)\n",
    "    hidden_size = 64\n",
    "    num_layers = 2\n",
    "    per_task_output_size = 2\n",
    "    overall_output_size = 2\n",
    "    dropout_prob = 0.3\n",
    "\n",
    "    model = HybridModelWithAttention(input_size, embedding_sizes, hidden_size, num_layers,\n",
    "                                     per_task_output_size, overall_output_size, dropout_prob)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "model = initialize_model(feature_columns, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(sequences, per_task_targets, overall_targets, batch_size=16):\n",
    "    \"\"\"\n",
    "    Creates training and validation DataLoaders.\n",
    "\n",
    "    Args:\n",
    "        sequences (list): List of task sequences.\n",
    "        per_task_targets (list): List of per-task target arrays.\n",
    "        overall_targets (list): List of overall target arrays.\n",
    "        batch_size (int): Batch size for DataLoaders.\n",
    "\n",
    "    Returns:\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        val_loader (DataLoader): DataLoader for validation data.\n",
    "    \"\"\"\n",
    "    dataset = TaskSequenceDataset(sequences, per_task_targets, overall_targets)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "train_loader, val_loader = create_dataloaders(sequences, per_task_targets, overall_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_functions_and_optimizer(model):\n",
    "    \"\"\"\n",
    "    Defines loss functions and optimizer for training.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to be trained.\n",
    "\n",
    "    Returns:\n",
    "        criterion_per_task (nn.Module): Loss function for per-task predictions.\n",
    "        criterion_overall (nn.Module): Loss function for overall predictions.\n",
    "        optimizer (optim.Optimizer): Optimizer for model parameters.\n",
    "    \"\"\"\n",
    "    criterion_per_task = nn.MSELoss()\n",
    "    criterion_overall = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    return criterion_per_task, criterion_overall, optimizer\n",
    "\n",
    "criterion_per_task, criterion_overall, optimizer = get_loss_functions_and_optimizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training Loss: 70.3729, Validation Loss: 42.5762\n",
      "Epoch [2/100], Training Loss: 25.3706, Validation Loss: 15.0666\n",
      "Epoch [3/100], Training Loss: 16.4723, Validation Loss: 13.9248\n",
      "Epoch [4/100], Training Loss: 15.2739, Validation Loss: 11.9666\n",
      "Epoch [5/100], Training Loss: 13.9387, Validation Loss: 11.3143\n",
      "Epoch [6/100], Training Loss: 13.6568, Validation Loss: 10.8003\n",
      "Epoch [7/100], Training Loss: 12.9649, Validation Loss: 10.5338\n",
      "Epoch [8/100], Training Loss: 12.8187, Validation Loss: 10.3457\n",
      "Epoch [9/100], Training Loss: 12.3011, Validation Loss: 10.2823\n",
      "Epoch [10/100], Training Loss: 12.3983, Validation Loss: 10.1727\n",
      "Epoch [11/100], Training Loss: 12.1433, Validation Loss: 10.2344\n",
      "Epoch [12/100], Training Loss: 12.5635, Validation Loss: 10.1404\n",
      "Epoch [13/100], Training Loss: 12.0089, Validation Loss: 10.2526\n",
      "Epoch [14/100], Training Loss: 12.0810, Validation Loss: 10.1308\n",
      "Epoch [15/100], Training Loss: 11.9058, Validation Loss: 10.1362\n",
      "Epoch [16/100], Training Loss: 11.5979, Validation Loss: 10.0652\n",
      "Epoch [17/100], Training Loss: 11.9693, Validation Loss: 10.0904\n",
      "Epoch [18/100], Training Loss: 11.5515, Validation Loss: 10.0667\n",
      "Epoch [19/100], Training Loss: 12.1257, Validation Loss: 10.0623\n",
      "Epoch [20/100], Training Loss: 11.7688, Validation Loss: 10.0617\n",
      "Epoch [21/100], Training Loss: 12.0377, Validation Loss: 10.0957\n",
      "Epoch [22/100], Training Loss: 11.8242, Validation Loss: 10.0942\n",
      "Epoch [23/100], Training Loss: 11.9705, Validation Loss: 10.1034\n",
      "Epoch [24/100], Training Loss: 12.0822, Validation Loss: 10.1264\n",
      "Epoch [25/100], Training Loss: 11.8816, Validation Loss: 10.1295\n",
      "Epoch [26/100], Training Loss: 11.9138, Validation Loss: 10.2137\n",
      "Epoch [27/100], Training Loss: 11.5738, Validation Loss: 10.1297\n",
      "Epoch [28/100], Training Loss: 11.7860, Validation Loss: 10.1495\n",
      "Epoch [29/100], Training Loss: 11.6299, Validation Loss: 10.0666\n",
      "Epoch [30/100], Training Loss: 11.7543, Validation Loss: 10.0865\n",
      "Epoch [31/100], Training Loss: 11.8590, Validation Loss: 10.0949\n",
      "Epoch [32/100], Training Loss: 11.6391, Validation Loss: 10.0717\n",
      "Epoch [33/100], Training Loss: 11.9550, Validation Loss: 10.0833\n",
      "Epoch [34/100], Training Loss: 12.2946, Validation Loss: 10.0154\n",
      "Epoch [35/100], Training Loss: 11.5790, Validation Loss: 10.0707\n",
      "Epoch [36/100], Training Loss: 11.8060, Validation Loss: 10.1700\n",
      "Epoch [37/100], Training Loss: 11.3823, Validation Loss: 10.1319\n",
      "Epoch [38/100], Training Loss: 11.4222, Validation Loss: 10.1699\n",
      "Epoch [39/100], Training Loss: 11.8377, Validation Loss: 10.1601\n",
      "Epoch [40/100], Training Loss: 11.5256, Validation Loss: 10.2727\n",
      "Epoch [41/100], Training Loss: 11.4158, Validation Loss: 10.2247\n",
      "Epoch [42/100], Training Loss: 11.3262, Validation Loss: 10.2911\n",
      "Epoch [43/100], Training Loss: 11.2950, Validation Loss: 10.3586\n",
      "Epoch [44/100], Training Loss: 11.1368, Validation Loss: 10.4388\n",
      "Epoch [45/100], Training Loss: 10.8981, Validation Loss: 10.5692\n",
      "Epoch [46/100], Training Loss: 11.2135, Validation Loss: 10.3734\n",
      "Epoch [47/100], Training Loss: 10.6973, Validation Loss: 10.9018\n",
      "Epoch [48/100], Training Loss: 10.6479, Validation Loss: 10.4608\n",
      "Epoch [49/100], Training Loss: 10.9153, Validation Loss: 10.7656\n",
      "Epoch [50/100], Training Loss: 10.7607, Validation Loss: 11.2732\n",
      "Epoch [51/100], Training Loss: 10.8515, Validation Loss: 11.3821\n",
      "Epoch [52/100], Training Loss: 10.5623, Validation Loss: 11.3116\n",
      "Epoch [53/100], Training Loss: 9.7779, Validation Loss: 11.5139\n",
      "Epoch [54/100], Training Loss: 9.7113, Validation Loss: 11.8139\n",
      "Epoch [55/100], Training Loss: 9.9372, Validation Loss: 11.6003\n",
      "Epoch [56/100], Training Loss: 9.9669, Validation Loss: 11.8887\n",
      "Epoch [57/100], Training Loss: 8.9085, Validation Loss: 12.4117\n",
      "Epoch [58/100], Training Loss: 9.0785, Validation Loss: 11.9294\n",
      "Epoch [59/100], Training Loss: 9.2188, Validation Loss: 12.1770\n",
      "Epoch [60/100], Training Loss: 8.9491, Validation Loss: 12.7916\n",
      "Epoch [61/100], Training Loss: 8.7248, Validation Loss: 12.1199\n",
      "Epoch [62/100], Training Loss: 8.9933, Validation Loss: 12.3671\n",
      "Epoch [63/100], Training Loss: 8.8192, Validation Loss: 12.0019\n",
      "Epoch [64/100], Training Loss: 8.8591, Validation Loss: 12.2689\n",
      "Epoch [65/100], Training Loss: 8.9422, Validation Loss: 12.1211\n",
      "Epoch [66/100], Training Loss: 8.8761, Validation Loss: 12.1598\n",
      "Epoch [67/100], Training Loss: 8.4302, Validation Loss: 12.2349\n",
      "Epoch [68/100], Training Loss: 8.4874, Validation Loss: 12.6767\n",
      "Epoch [69/100], Training Loss: 8.1985, Validation Loss: 12.2048\n",
      "Epoch [70/100], Training Loss: 8.2211, Validation Loss: 12.7085\n",
      "Epoch [71/100], Training Loss: 8.1015, Validation Loss: 13.1414\n",
      "Epoch [72/100], Training Loss: 8.4115, Validation Loss: 12.4857\n",
      "Epoch [73/100], Training Loss: 8.1205, Validation Loss: 13.2473\n",
      "Epoch [74/100], Training Loss: 7.7802, Validation Loss: 12.8708\n",
      "Epoch [75/100], Training Loss: 7.9879, Validation Loss: 12.8280\n",
      "Epoch [76/100], Training Loss: 8.2171, Validation Loss: 12.7388\n",
      "Epoch [77/100], Training Loss: 8.2885, Validation Loss: 12.6839\n",
      "Epoch [78/100], Training Loss: 8.0259, Validation Loss: 13.1044\n",
      "Epoch [79/100], Training Loss: 7.7200, Validation Loss: 13.1840\n",
      "Epoch [80/100], Training Loss: 7.9055, Validation Loss: 13.3369\n",
      "Epoch [81/100], Training Loss: 7.7921, Validation Loss: 13.5217\n",
      "Epoch [82/100], Training Loss: 7.6513, Validation Loss: 13.0004\n",
      "Epoch [83/100], Training Loss: 7.7330, Validation Loss: 13.2422\n",
      "Epoch [84/100], Training Loss: 7.7670, Validation Loss: 12.9057\n",
      "Epoch [85/100], Training Loss: 7.3627, Validation Loss: 13.3543\n",
      "Epoch [86/100], Training Loss: 7.7299, Validation Loss: 13.2485\n",
      "Epoch [87/100], Training Loss: 7.3209, Validation Loss: 13.1530\n",
      "Epoch [88/100], Training Loss: 7.5086, Validation Loss: 13.6535\n",
      "Epoch [89/100], Training Loss: 7.3156, Validation Loss: 13.3486\n",
      "Epoch [90/100], Training Loss: 7.4350, Validation Loss: 13.1810\n",
      "Epoch [91/100], Training Loss: 7.3590, Validation Loss: 13.5796\n",
      "Epoch [92/100], Training Loss: 7.3796, Validation Loss: 13.8783\n",
      "Epoch [93/100], Training Loss: 7.3016, Validation Loss: 14.5680\n",
      "Epoch [94/100], Training Loss: 7.3634, Validation Loss: 13.8222\n",
      "Epoch [95/100], Training Loss: 7.3046, Validation Loss: 14.4526\n",
      "Epoch [96/100], Training Loss: 7.4929, Validation Loss: 13.9589\n",
      "Epoch [97/100], Training Loss: 7.4794, Validation Loss: 13.7595\n",
      "Epoch [98/100], Training Loss: 7.0887, Validation Loss: 13.6312\n",
      "Epoch [99/100], Training Loss: 7.0785, Validation Loss: 13.1031\n",
      "Epoch [100/100], Training Loss: 6.9912, Validation Loss: 13.5058\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion_per_task, criterion_overall, optimizer, num_epochs=20):\n",
    "    \"\"\"\n",
    "    Trains the model for the specified number of epochs.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to be trained.\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        val_loader (DataLoader): DataLoader for validation data.\n",
    "        criterion_per_task (nn.Module): Loss function for per-task predictions.\n",
    "        criterion_overall (nn.Module): Loss function for overall predictions.\n",
    "        optimizer (optim.Optimizer): Optimizer for model parameters.\n",
    "        num_epochs (int): Number of epochs to train.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            sequences = batch['sequence'].to(device)\n",
    "            per_task_targets = batch['per_task_target'].to(device)\n",
    "            overall_targets = batch['overall_target'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            per_task_outputs, overall_output = model(sequences)\n",
    "\n",
    "            per_task_outputs_flat = per_task_outputs.view(-1, per_task_outputs.shape[-1])\n",
    "            per_task_targets_flat = per_task_targets.view(-1, per_task_targets.shape[-1])\n",
    "\n",
    "            loss_per_task = criterion_per_task(per_task_outputs_flat, per_task_targets_flat)\n",
    "            loss_overall = criterion_overall(overall_output, overall_targets)\n",
    "\n",
    "            loss = loss_per_task + loss_overall\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                sequences = batch['sequence'].to(device)\n",
    "                per_task_targets = batch['per_task_target'].to(device)\n",
    "                overall_targets = batch['overall_target'].to(device)\n",
    "\n",
    "                per_task_outputs, overall_output = model(sequences)\n",
    "\n",
    "                per_task_outputs_flat = per_task_outputs.view(-1, per_task_outputs.shape[-1])\n",
    "                per_task_targets_flat = per_task_targets.view(-1, per_task_targets.shape[-1])\n",
    "\n",
    "                loss_per_task = criterion_per_task(per_task_outputs_flat, per_task_targets_flat)\n",
    "                loss_overall = criterion_overall(overall_output, overall_targets)\n",
    "\n",
    "                loss = loss_per_task + loss_overall\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {running_loss/len(train_loader):.4f}, Validation Loss: {val_loss/len(val_loader):.4f}')\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion_per_task, criterion_overall, optimizer, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, file_path):\n",
    "    \"\"\"\n",
    "    Saves the trained model to the specified file.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model.\n",
    "        file_path (str): Path to save the model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), file_path)\n",
    "\n",
    "save_model(model, 'hybrid_model_with_attention.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
